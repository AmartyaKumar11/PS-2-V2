{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network for Resume-Job Description Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "RESUME_PATH = '../data/raw/parsed_resumes.csv'\n",
    "JD_FOLDER_PATH = '../data/job_descriptions/'\n",
    "OUTPUT_PATH = '../data/processed/triplet_training_data.csv'\n",
    "\n",
    "# Define the target job description for positive examples\n",
    "TARGET_JD_FILENAME = 'Web-Developer-job-description.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2437 resumes. Created Resume_str by concatenating: ['Person Name', 'Work Experience', 'Skills', 'Education']\n"
     ]
    }
   ],
   "source": [
    "# Load Resumes\n",
    "resumes_df = pd.read_csv(RESUME_PATH)\n",
    "\n",
    "# Concatenate relevant fields to create Resume_str\n",
    "fields_to_concat = [\n",
    "    'Person Name', 'Work Experience', 'Skills', 'Education', 'Certifications', 'Projects', 'Summary', 'Contact Information'\n",
    "]\n",
    "\n",
    "# Only use fields that exist in the dataframe\n",
    "fields_to_concat = [f for f in fields_to_concat if f in resumes_df.columns]\n",
    "\n",
    "resumes_df['Resume_str'] = resumes_df[fields_to_concat].fillna('').agg(' '.join, axis=1)\n",
    "\n",
    "print(f'Loaded {len(resumes_df)} resumes. Created Resume_str by concatenating: {fields_to_concat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 job descriptions.\n"
     ]
    }
   ],
   "source": [
    "# Load Job Descriptions\n",
    "jd_files = os.listdir(JD_FOLDER_PATH)\n",
    "job_descriptions = {}\n",
    "for file_name in jd_files:\n",
    "    with open(os.path.join(JD_FOLDER_PATH, file_name), 'r', encoding='utf-8') as f:\n",
    "        job_descriptions[file_name] = f.read()\n",
    "\n",
    "print(f'Loaded {len(job_descriptions)} job descriptions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Triplets\n",
    "We will create a triplet for each resume. The resume is the **anchor**. The **positive** example is the target job description ('Web-Developer-job-description.txt'). The **negative** example is any other job description chosen at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:00<00:00, 62584.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2437 triplets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anchor</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A senior systems administrator trico products ...</td>\n",
       "      <td>\\nJob Title: Web Developer\\nCompany: Not speci...</td>\n",
       "      <td>Position: Data Scientist\\nExperience: 2-4 Year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B systems administrator bios technologies - me...</td>\n",
       "      <td>\\nJob Title: Web Developer\\nCompany: Not speci...</td>\n",
       "      <td>Position: Software Engineer\\nExperience: 1-3 Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C systems administrator nord gear corporation ...</td>\n",
       "      <td>\\nJob Title: Web Developer\\nCompany: Not speci...</td>\n",
       "      <td>Position: Project Manager\\nExperience: 3-6 Yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D roti mediterranean grill - north bethesda, m...</td>\n",
       "      <td>\\nJob Title: Web Developer\\nCompany: Not speci...</td>\n",
       "      <td>Position: Data Scientist\\nExperience: 2-4 Year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E systems administrator bex realty - boca rato...</td>\n",
       "      <td>\\nJob Title: Web Developer\\nCompany: Not speci...</td>\n",
       "      <td>Position: Data Analyst\\nExperience: 2-5 Years\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              anchor  \\\n",
       "0  A senior systems administrator trico products ...   \n",
       "1  B systems administrator bios technologies - me...   \n",
       "2  C systems administrator nord gear corporation ...   \n",
       "3  D roti mediterranean grill - north bethesda, m...   \n",
       "4  E systems administrator bex realty - boca rato...   \n",
       "\n",
       "                                            positive  \\\n",
       "0  \\nJob Title: Web Developer\\nCompany: Not speci...   \n",
       "1  \\nJob Title: Web Developer\\nCompany: Not speci...   \n",
       "2  \\nJob Title: Web Developer\\nCompany: Not speci...   \n",
       "3  \\nJob Title: Web Developer\\nCompany: Not speci...   \n",
       "4  \\nJob Title: Web Developer\\nCompany: Not speci...   \n",
       "\n",
       "                                            negative  \n",
       "0  Position: Data Scientist\\nExperience: 2-4 Year...  \n",
       "1  Position: Software Engineer\\nExperience: 1-3 Y...  \n",
       "2  Position: Project Manager\\nExperience: 3-6 Yea...  \n",
       "3  Position: Data Scientist\\nExperience: 2-4 Year...  \n",
       "4  Position: Data Analyst\\nExperience: 2-5 Years\\...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_jd = job_descriptions[TARGET_JD_FILENAME]\n",
    "negative_jd_files = [f for f in jd_files if f != TARGET_JD_FILENAME]\n",
    "\n",
    "triplets = []\n",
    "for index, row in tqdm(resumes_df.iterrows(), total=resumes_df.shape[0]):\n",
    "    # Anchor is the resume text\n",
    "    anchor = row['Resume_str']\n",
    "    \n",
    "    # Positive is the target JD\n",
    "    positive = positive_jd\n",
    "    \n",
    "    # Negative is a randomly chosen different JD\n",
    "    negative_filename = random.choice(negative_jd_files)\n",
    "    negative = job_descriptions[negative_filename]\n",
    "    \n",
    "    triplets.append({'anchor': anchor, 'positive': positive, 'negative': negative})\n",
    "\n",
    "triplets_df = pd.DataFrame(triplets)\n",
    "print(f'Generated {len(triplets_df)} triplets.')\n",
    "triplets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet data saved to ../data/processed/triplet_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the triplets to a new CSV file\n",
    "triplets_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f'Triplet data saved to {OUTPUT_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2 & 3: Model Architecture & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "MARGIN = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        anchor = self.tokenizer(row['anchor'], max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        positive = self.tokenizer(row['positive'], max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        negative = self.tokenizer(row['negative'], max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'anchor': {'input_ids': anchor['input_ids'].flatten(), 'attention_mask': anchor['attention_mask'].flatten()}, \n",
    "            'positive': {'input_ids': positive['input_ids'].flatten(), 'attention_mask': positive['attention_mask'].flatten()}, \n",
    "            'negative': {'input_ids': negative['input_ids'].flatten(), 'attention_mask': negative['attention_mask'].flatten()}\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Siamese Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.encoder = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward_once(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use mean pooling for sentence representation\n",
    "        pooled_output = outputs.last_hidden_state.mean(axis=1)\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor_embedding = self.forward_once(anchor['input_ids'], anchor['attention_mask'])\n",
    "        positive_embedding = self.forward_once(positive['input_ids'], positive['attention_mask'])\n",
    "        negative_embedding = self.forward_once(negative['input_ids'], negative['attention_mask'])\n",
    "        return anchor_embedding, positive_embedding, negative_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
    "        loss = torch.mean(F.relu(distance_positive - distance_negative + self.margin))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a367c54a0eae4314bd17c71033fc2602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AMARTYA KUMAR\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AMARTYA KUMAR\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fc673b7a9d464699e930338ea0d5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6451ac9d562465d868bc23681b8a07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb53e55922c48f682fe9a2db1bf6dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4e94ea738543a98fded0ad3c60718d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_dataset = TripletDataset(triplets_df, tokenizer, MAX_LENGTH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork(MODEL_NAME).to(device)\n",
    "loss_fn = TripletLoss(margin=MARGIN)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f'Training on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 153/153 [32:12<00:00, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        anchor = {k: v.to(device) for k, v in batch['anchor'].items()}\n",
    "        positive = {k: v.to(device) for k, v in batch['positive'].items()}\n",
    "        negative = {k: v.to(device) for k, v in batch['negative'].items()}\n",
    "        \n",
    "        anchor_embedding, positive_embedding, negative_embedding = model(anchor, positive, negative)\n",
    "        \n",
    "        loss = loss_fn(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}, Average Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ENCODER_SAVE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/siamese_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(ENCODER_SAVE_PATH, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39msave_pretrained(ENCODER_SAVE_PATH)\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(ENCODER_SAVE_PATH)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel encoder saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mENCODER_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AMARTYA KUMAR\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3937\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   3932\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m   3934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[0;32m   3935\u001b[0m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[0;32m   3936\u001b[0m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[1;32m-> 3937\u001b[0m     safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m   3938\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3939\u001b[0m     save_function(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file))\n",
      "File \u001b[1;32mc:\\Users\\AMARTYA KUMAR\\anaconda3\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    256\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    257\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    258\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m ):\n\u001b[0;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m     serialize_file(_flatten(tensors), filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[1;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "ENCODER_SAVE_PATH = '../models/siamese_encoder'\n",
    "os.makedirs(ENCODER_SAVE_PATH, exist_ok=True)\n",
    "model.encoder.save_pretrained(ENCODER_SAVE_PATH, safe_serialization=False)\n",
    "tokenizer.save_pretrained(ENCODER_SAVE_PATH)\n",
    "print(f'Model encoder saved to {ENCODER_SAVE_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Inference and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Fine-Tuned Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and in evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "ENCODER_PATH = '../models/siamese_encoder'\n",
    "RANKING_OUTPUT_PATH = '../data/results/siamese_ranking_results.csv'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(ENCODER_PATH)\n",
    "encoder = DistilBertModel.from_pretrained(ENCODER_PATH)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "print('Model loaded and in evaluation mode.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, tokenizer, model, device, max_length=256):\n",
    "    inputs = tokenizer(text, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs).last_hidden_state.mean(axis=1)\n",
    "    return output.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings for Job Description and Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embedding for the target job description.\n"
     ]
    }
   ],
   "source": [
    "# Get the target job description text\n",
    "target_jd_text = job_descriptions[TARGET_JD_FILENAME]\n",
    "\n",
    "# Generate embedding for the JD\n",
    "jd_embedding = get_embedding(target_jd_text, tokenizer, encoder, device)\n",
    "print('Generated embedding for the target job description.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [03:01<00:00, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 2437 resumes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all resumes\n",
    "resumes_df['embedding'] = resumes_df['Resume_str'].progress_apply(\n",
    "    lambda x: get_embedding(x, tokenizer, encoder, device)\n",
    ")\n",
    "print(f'Generated embeddings for {len(resumes_df)} resumes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Similarity and Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2437/2437 [00:00<00:00, 45513.32it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "resumes_df['similarity_score'] = resumes_df['embedding'].progress_apply(\n",
    "    lambda x: F.cosine_similarity(x, jd_embedding).item()\n",
    ")\n",
    "\n",
    "# Sort by similarity score\n",
    "ranked_resumes = resumes_df.sort_values(by='similarity_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking complete. Results saved to ../data/results/siamese_ranking_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>ADR front-end software developer ecra group, i...</td>\n",
       "      <td>0.928677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>XT front-end developer hack illinois august 20...</td>\n",
       "      <td>0.926955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>BQN  ? ? extensive ms office tools (word, exce...</td>\n",
       "      <td>0.923196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>XK senior front end web developer oogloo.com f...</td>\n",
       "      <td>0.921070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>ADO front end ui developer nwea/hitachi - port...</td>\n",
       "      <td>0.920999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Resume_str  similarity_score\n",
       "797   ADR front-end software developer ecra group, i...          0.928677\n",
       "643   XT front-end developer hack illinois august 20...          0.926955\n",
       "1807  BQN  ? ? extensive ms office tools (word, exce...          0.923196\n",
       "634   XK senior front end web developer oogloo.com f...          0.921070\n",
       "794   ADO front end ui developer nwea/hitachi - port...          0.920999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the results\n",
    "final_ranking = ranked_resumes[['Resume_str', 'similarity_score']]\n",
    "final_ranking.to_csv(RANKING_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f'Ranking complete. Results saved to {RANKING_OUTPUT_PATH}')\n",
    "final_ranking.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
